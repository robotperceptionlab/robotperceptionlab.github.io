<html>
<head>
    <title>ICRA 2019 ViTac Workshop</title>
    <meta name="date" content="2019-01-02 00:00"/>
    <meta name="category" content="ICRA Vitac Workshop"/>
    <meta name="picture" content="articles/icra-2019-vitac-workshop/L1040227.jpg"/>
</head>
<body>

<h3>ViTac: Integrating Vision and Touch for Multimodal and Cross-modal Perception</h3>
<h5><a class="fl"
       href="https://www.google.co.uk/search?sa=X&amp;biw=1467&amp;bih=1196&amp;q=Palais+des+congr%C3%A8s+de+Montr%C3%A9al&amp;stick=H4sIAAAAAAAAAOPgE-LVT9c3NEzPKDTLSDM0UOLUz9U3MItPyjDWEs5OttJPLUvNKym2yslPTizJzM9bxKoQkJiTmFmskJJarJCcn5dedHgFiKPgm59XUnR4ZWIOAFvY0WtUAAAA&amp;ved=2ahUKEwi6tJ-TtJDgAhVUuXEKHSvZDMoQmxMoATAOegQIAxAH"
       data-ved="2ahUKEwi6tJ-TtJDgAhVUuXEKHSvZDMoQmxMoATAOegQIAxAH">Montreal Convention Centre, Montreal, Canada</a>
</h5>
<h5>23 May 2019</h5>
<h3>
    <span style="color: #000000;">
        <strong>Detailed program available <span style="color: #3366ff;">
    <a style="color: #3366ff;"
       href="icra-2019-vitac-workshop-program.html">here</a></span>.</strong></span>
</h3>
<h3><span style="color: #000000;"><strong>Accepted papers available <span style="color: #3366ff;">
    <a style="color: #3366ff;"
       href="icra-2019-vitac-workshop-accepted-papers.html">here</a></span>.</strong></span>
</h3>
<!--<p><img loading="lazy"-->
<!--        src="{static}L1040227.jpg"-->
<!--        width="604"-->
<!--        height="453"></p>-->
<p>In this one-day workshop we will bring together experts from the diverse range of disciplines and encompass
    engineers, computer scientists, cognitive scientists and sensor developers, to discuss topics relating to fusion of
    vision and touch sensing.</p>


<p><strong>Scope </strong></p>

<p>The workshop will be divided into three sessions, each of 6 standard talks. Our intention is to separate the sessions
    into three themes: development of touch sensors using optical sensing for cross-modal perception (hardware
    focussed); multimodal robot perception using vision and tactile sensing; inspirations from how humans integrate
    visual and haptic information. The sessions will be followed by breaks for refreshments, posters and live
    demonstrations that will encourage interaction among attendees. </p>


<h6><strong>Topics of interest</strong></h6>
<ul>
    <li>trends in combining vision and tactile sensing for robot perception</li>
    <li>development of optical tactile sensors (using visual cameras or optical fibres)</li>
    <li>integration of optical tactile sensors into robotic grippers and hands</li>
    <li>roles of vision and touch sensing in different object perception tasks, e.g., object recognition, localization,
        object exploration, planning, learning and action selection
    </li>
    <li>interplay between touch sensing and vision</li>
    <li>bio-inspired approaches for fusion of vision and touch sensing</li>
    <li>psychophysics and neuroscience of combining vision and tactile sensing in humans and animals</li>
    <li>computational methods for processing vision and touch data in robot learning</li>
    <li>deep learning for optical tactile sensing and relation/interaction with deep learning for robot vision</li>
    <li>the use of vision and touch for safe human-robot interaction/collaboration</li>
</ul>


<h6>Invited Speakers</h6>
<ul>
    <li><strong>Edward Adelson (MIT)</strong> – world-renowned neuroscientist in vision science and is also well known
        for his development of the GelSight optical touch&nbsp;sensor;
    </li>
    <li><strong>Peter Allen (Columbia University)</strong> – recognised for his multiple pioneering works on integration
        of vision and tactile sensing, especially applied in robot grasping;
    </li>
    <li><strong>Yasemin Bekiroglu (Vicarious AI, USA)</strong> – an expert in using vision and touch data for grasp
        stability assessment;
    </li>
    <li><strong>Alberto Rodriguez (MIT)</strong> – an expert in grasping and manipulation, won several Amazon picking
        challenges, development of the GelSlim touch sensor recently;
    </li>
    <li><strong>Oliver Kroemer (CMU)</strong>&nbsp; – renowned for his pioneering work on learning dynamic tactile
        sensing with vision-based training;
    </li>
    <li><strong>Sergey Levine (UC Berkeley)</strong> – an expert in machine learning and reinforcement learning, with
        applications in hand-eye coordination for robotic grasping;
    </li>
    <li><strong>Lorenzo Natale (IIT)</strong> – an expert in tactile and visual perception, especially applied in
        humanoid robotics;
    </li>
    <li><strong>Vincent Hayward (UPMC Univ Paris)</strong>&nbsp; – distinguished and well known for his research on
        human perception, especially human touch and haptics;
    </li>
    <li><strong>Rebecca&nbsp;Lawson (University of Liverpool)</strong>&nbsp; – an expert in diverse aspects of
        processing in human object recognition system for vision and for haptics;
    </li>
    <li><strong>Hongbin Liu (King’s College London)</strong> – an expert in developing optical (fibre) based tactile
        sensors and soft robotics.
    </li>
</ul>


<p><strong>Key dates</strong></p>
<p>Posters and the live demonstrations will be selected from call for extended abstracts, reviewed by the organizers.
    The best posters will be invited to talk at the workshop. All submissions will be reviewed using a single-blind
    review process. Accepted contributions will be presented during the workshop as posters. Expected contributions
    should be submitted in the form of extended abstracts (max 2 pages) in IEEE Conference paper format. Submissions
    should be in PDF format (&lt;5MB), following the IEEE conference style (two-columns), <s>via the EasyChair
        submission page</s>. <em>(Submissions are now closed)</em></p>


<p class="has-vivid-red-color has-text-color">Submission Deadline:
    <del>1 April, 2019</del>
    <strong>15 April, 2019</strong></p>

<p>Notification of acceptance: 30 April, 2019</p>
<p>Camera-ready deadline: 15 May, 2019 </p>
<p><strong><s>(Submit your camera-ready paper by sending an email to shan.luo@liverpool.ac.uk)</s></strong></p>

<p>Workshop day: 23 May, 2019 (whole day)</p>
<p>Contact: shan.luo@liverpool.ac.uk</p>

<h4><strong>News</strong></h4>

<p>The accepted papers at the ViTac workshop are invited to submit an extended full-paper to the Special Issue at the
    Frontiers in Robotics and AI on the same topic. More details can be found <a
            href="https://www.frontiersin.org/research-topics/10004/vitac-integrating-vision-and-touch-for-multimodal-and-cross-modal-perception">here</a>.
</p>


<h6>Organisers</h6>


<p><a href="https://cgi.csc.liv.ac.uk/~shanluo/">Shan Luo</a> (University of Liverpool)</p>
<p>
    <a href="http://www.google.com/url?q=http%3A%2F%2Fwww.lepora.com&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHlV_iTXkI0CCb_-QktFexJwuGFwA"
       target="_blank" rel="noreferrer noopener">Nathan Lepora</a> (Univ Bristol &amp; Bristol Robotics Lab)</p>


<p><a href="https://researchportal.bath.ac.uk/en/persons/uriel-martinez-hernandez">Uriel Martinez Hernandez</a>
    (University of Bath)</p>
<p><a href="https://www.iit.it/people/joao-bimbo">João Bimbo</a>&nbsp;(Istituto Italiano di Tecnologia)</p>
<p><a href="https://sites.google.com/site/thuliuhuaping/">Huaping Liu</a> (Tsinghua University)</p>
<p><strong>We are grateful for the support of the following organisations</strong></p>

<ul>
    <li><a href="https://www.ieee-ras.org/haptics">IEEE RAS Technical Committee on Haptics</a></li>
    <li><a href="https://www.ieee-ras.org/bio-robotics">IEEE RAS Technical Committee on BioRobotics</a></li>
    <li><a href="https://www.ieee-ras.org/human-robot-interaction-coordination">IEEE RAS Technical Committee on
        Human-Robot Interaction and Coordination</a></li>
    <li><a href="https://www.ieee-ras.org/computer-robot-vision">IEEE RAS Technical Committee on Computer and Robot
        Vision</a></li>
    <li><a href="https://www.ieee-ras.org/cognitive-robotics">IEEE RAS Technical Committee on Cognitive Robotics</a>
    </li>
    <li><a href="https://rainhub.org.uk/">EPSRC Robotics and AI in Nuclear (RAIN) Research Hub</a></li>
</ul>
</body>
</html>