<html>
<head>
    <title>ICRA 2020 ViTac Workshop</title>
    <meta name="date" content="2020-01-02 00:00"/>
    <meta name="category" content="ICRA Vitac Workshop"/>
</head>
<body>

<h3><strong>ViTac 2020: Closing the Perception-Action Loop with Vision and Tactile Sensing</strong></h3>

<h5>Held online via Zoom <br>23 May 2020</h5>
<h3>
    <strong>Detailed program available <a href="icra-2020-vitac-workshop-program.html">here</a>.</strong>
</h3>


<h3><strong>Accepted papers available <a href="icra-2020-vitac-workshop-accepted-papers.html">here</a>.</strong></h3>


<p><strong>The workshop will take place online (live) via Zoom.</strong></p>


<p><s>To participate in the workshop, please register <strong>here</strong> for the Zoom meeting.</s></p>


<p>Participation will be <strong>free of charge</strong>. </p>


<p>Workshop Slack channel to ask questions to speakers in addition to live discussions.</p>


<p><strong>Recordings of the workshop have been uploaded to </strong><a
        href="https://www.youtube.com/channel/UCc8Ro4QD0URX2jNqfs8pbbA?view_as=subscriber"><strong>our YouTube
    channel</strong></a><strong>.</strong> We had around 500 registrations!</p>


<p><strong>The chat texts in the meeting: </strong><a
        href="{static}chat-texts/ICRA-2020-1.pdf" data-type="URL"
        data-id="{static}chat-texts/ICRA-2020-1.pdf"><strong>morning
    session</strong></a><strong> and </strong><a
        href="{static}chat-texts/ICRA-2020-2.pdf" data-type="URL"
        data-id="{static}chat-texts/ICRA-2020-2.pdf"><strong>afternoon
    session.</strong></a><strong> </strong></p>


<p>This workshop will encompass recent progress in the area of combining vision and touch sensing for integrated
    perception- action loop. The full-day workshop aims to enhance active collaboration, discussion of methods for the
    fusion of vision and touch in both perception and action, challenges for this important topic and applications. </p>


<h4>Scope</h4>


<p>The workshop will be divided into three themes: development of touch sensors for perception-action tasks (hardware
    focused); multimodal robot perception and action using vision and tactile sensing; and inspirations from how humans
    integrate visual and haptic information for perception-action tasks. <br></p>


<h4>Topics of Interest</h4>


<ul>
    <li>trends in combining visual and tactile sensing for robot perception and actions</li>
    <li>development of optical tactile sensors (using visual cameras or optical fibres)</li>
    <li>integration of tactile sensing and vision for robot tasks, e.g., manipulation and grasping</li>
    <li>roles of vision and touch sensing in different tasks, e.g., object recognition, localization, object
        exploration, planning, learning and action selection
    </li>
    <li>interplay between perception and actions with touch sensing and vision</li>
    <li>bio-inspired approaches for fusion of vision and touch sensing in perception and actions</li>
    <li>psychophysics and neuroscience of combining vision and tactile sensing in humans and animals</li>
    <li>computational methods for processing vision and touch data in robot learning</li>
    <li>deep learning for optical tactile sensing and relation/interaction with deep learning for robot vision</li>
    <li>the use of vision and touch for safe human-robot interaction/collaboration</li>
</ul>


<h4>Invited Speakers </h4>


<ul>
    <li><strong>Peter Allen (Columbia University)</strong> – recognised for his multiple pioneering works on integration
        of vision and tactile sensing, especially applied in robot grasping;
    </li>
    <li><strong>Dieter Fox (University of Washington and Nvidia)</strong> – world-renowned roboticist, there have been
        quite a few research works on using vision and tactile sensing for different tasks at his prestigious research
        group;
    </li>
    <li><strong>Vincent Hayward (UPMC Univ Paris)</strong> – distinguished and well known for his research on human
        perception, especially human touch and haptics;
    </li>
    <li><strong>Alberto Rodriguez (MIT)</strong> – an expert in grasping and manipulation, won several Amazon picking
        challenges, with the assistance of both vision and tactile sensing; <a
                href="https://www.robertocalandra.com/about/"></a></li>
    <li><strong>Roberto Calandra (Facebook)</strong> – an expert in machine learning and reinforcement learning, with
        applications in tactile sensing and dynamics modeling;
    </li>
    <li><strong>Kaspar Althoefer (Queen Mary University of London)</strong> – an expert in developing optical (fibre)
        based tactile sensors and soft robotics for different applications;
    </li>
    <li><strong>Huaping Liu (Tsinghua University) </strong>– recognised for his contributions to the multimodal
        perception with vision and tactile sensing;
    </li>
    <li><strong>Robert Haschke (Bielefeld University)</strong> – a renowned expert for the works on tactile-servoing
        based manipulation
    </li>
    <li><strong>Lorenzo Natale (Istituto Italiano di Tecnologia)</strong> – recognised for his pioneering work in active
        touch and visual perception on the iCub robot
    </li>
</ul>


<p><strong>Key Dates</strong></p>


<p>Posters and live demonstrations will be selected from call for extended abstracts, reviewed by the organizers and
    invited reviewers. The best posters will be invited to talk at the workshop. All submissions will be reviewed using
    a single-blind review process. Accepted contributions will be presented during the workshop as posters. <strong>Submissions
        must be sent in pdf, following the IEEE conference style (two-columns and must not exceed two pages)</strong>,
    to the EasyChair system. </p>


<ul>
    <li><strong>Submission Deadline: 1st April, 2020</strong></li>
    <li>Submission link: <em>(submissions now closed)</em></li>
    <li>Notification of acceptance: 30th March, 2020</li>
    <li>Camera-ready deadline: 15th April, 2020</li>
    <li> Workshop day: 31st May 2020</li>
</ul>


<h4>Organizers</h4>


<p><a href="https://cgi.csc.liv.ac.uk/~shanluo/">Shan Luo</a> (University of Liverpool)</p>


<p><a rel="noreferrer noopener"
      href="http://www.google.com/url?q=http%3A%2F%2Fwww.lepora.com&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHlV_iTXkI0CCb_-QktFexJwuGFwA"
      target="_blank">Nathan Lepora</a> (Univ Bristol &amp; Bristol Robotics Lab)</p>


<p><a href="https://www.ri.cmu.edu/ri-faculty/wenzhen-yuan/">Wenzhen Yuan</a> (Carnegie Mellon University)</p>


<p><a href="http://www.ics.ei.tum.de/en/people/cheng/">Gordon Cheng</a> (Technische Universität München)<br></p>


<p><strong>We are grateful for the support of the following organisations</strong></p>


<ul>
    <li><a href="https://www.ieee-ras.org/haptics">IEEE RAS Technical Committee on Haptics</a></li>
    <li><a href="https://www.ieee-ras.org/human-robot-interaction-coordination">IEEE RAS Technical Committee on
        Human-Robot Interaction and Coordination</a></li>
    <li><a href="https://www.ieee-ras.org/computer-robot-vision">IEEE RAS Technical Committee on Computer and Robot
        Vision</a></li>
    <li><a href="https://www.ieee-ras.org/cognitive-robotics">IEEE RAS Technical Committee on Cognitive Robotics</a>
    </li>
</ul>
</body>
</html>